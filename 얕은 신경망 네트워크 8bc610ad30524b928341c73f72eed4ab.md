# 얕은 신경망 네트워크

## 구성

- 은닉층 : 입력층과 출력층 사이에 있는 모든 층
    - 은닉층의 값들은 (훈련 세트에서)알 수 없음
    - 은닉층과 출력층은 연관된 매개변수가 있다.

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled.png)

- 출력층 : 출력 특성들의 층
- 신경망의 층의 개수에서 입력층은 제외된다.

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%201.png)

## 활성화 함수 (Activation Functions)

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%202.png)

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%203.png)

- Tanh함수는 대부분의 경우에서 Sigmoid 보다 좋다.
    - Tanh [-1,1] 사이에 값이 있고 평균값이 0에 더 가깝다. 데이터 중심를 원점으로 이동하는 효과가 있다.
- 출력층은 예외로 [0, 1]사이의 값으로 표현되는 것이 좋다.
    - 그래서 Sigmoid는 이진분류에 사용된다.
- sigmoid와 tanh는 z가 굉장히 크거나 작으면 함수의 도함수가 굉장히 작아져 학습 속도가 떨어진다는 단점이 있다.

- ReLu
    - 정류 선형 유닛(rectified linear unit)
    - 양수일 때 도함수 1 음수일 떄 0
    - 활성화 함수의 기본값으로 많이 사용된다.
    - 0보다 큰 활성화 함수의 미분값이 다른 함수에 비해 많아서 **빠르게 학습** 할 수 있다.
        - 학습을 느리게하는 원인 함수의 기울기가 0에 가까워지는 것
        - z의 절반에 대해 ReLU의 기울기가 0이지만 실제로 충분한 은닉 유닛의 z는 0보다 크기 때문에 실제로는 잘 동작한다.
        

### 왜 비선형 활성화 함수를 사용해야하는가

- 선형 활성화 샇ㅁ수나 항등 활성화 함수를 사용한다면 입력의 선형식만 출력한다. 층이 얼마나 많던 간에 신경망은 선형 활성화 함수만 계산하기 때문에 은닉층이 없는 것과 같음.
- 선형 활성화 함수를 쓸 수 있는 곳은 대부분 출력층이다.
- 회귀 문제에 대한 머신러닝을 할 때 선형함수가 쓰임 (y가 실수값일 때 : 집값 예측)

### 활성화 함수의 미분

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%204.png)

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%205.png)

## Gradient descent 경사하강법

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%206.png)

- 신경망을 훈련시킬 때 0이 아닌 값으로 변수를 초기화하는 것이 중요하다

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%207.png)

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%208.png)

- 역전파가 잘 되었는지 확인하는 방법은 차원을 확인 해보면 오류를 미리 제거할 수 있다.
- 실제로는 여러 훈련 샘플을 벡터화하여 처리한다.

![Untitled](%E1%84%8B%E1%85%A3%E1%87%80%E1%84%8B%E1%85%B3%E1%86%AB%20%E1%84%89%E1%85%B5%E1%86%AB%E1%84%80%E1%85%A7%E1%86%BC%E1%84%86%E1%85%A1%E1%86%BC%20%E1%84%82%E1%85%A6%E1%84%90%E1%85%B3%E1%84%8B%E1%85%AF%E1%84%8F%E1%85%B3%208bc610ad30524b928341c73f72eed4ab/Untitled%209.png)

### 훈련시킬 때 변수 초기화

- 신경망에서 w의 초기값을 0으로 설정한 후 경사하강법을 적용할 경우 올바르게 작동하지 않는다.
    - 어떠한 샘플의 경우에서도 a^[1]_1과 a^[1]_2가 같은 값을 가진다. → **활성이 같아짐 complete symetric**
    - dz^[1]의 값들이 같아짐 → 가중치의 결과 값이 같음 w^[2] = [0, 0]
    - dw의 각 열이 같은 값을 가진다.
    - 가중치를 계산 각 반복 이후 w^[1]의 첫번째 열이 두번쨰 열과 같아지게 된다.
- 가중치의 초기값을매우 작은 값(*0.01)로 정하는 것이 좋다.
    - 활성 함수를 사용할 때 가중치가 너무 큰 값을 가지는 경우 z는 아주 큰 값 혹은 작은 값이 되며 경사의 기울기가 매우 낮으며 학습이 매우 느리다.